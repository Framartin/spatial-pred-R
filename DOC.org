#+TITLE:       Extending Predictions from Spatial Econometric Models on R
#+AUTHOR:      Jean-Sauveur AY \\ \lt[[mailto:jsay.site@gmail.com][jsay.site@gmail.com]]\gt \and Julie LE GALLO\\ \lt[[mailto:jlegallo@univ-fcomte.fr][jlegallo@univ-fcomte.fr]]\gt 
#+LaTeX_CLASS: ManueStat
#+OPTIONS:     LaTeX:t tags:nil toc:nil H:5
#+STARTUP:     hideblocks
#+BIND:        org-latex-image-default-width ""
#+BIND:        org-latex-tables-booktabs t
#+PROPERTY:    session *R*
#+PROPERTY:    exports both
#+PROPERTY:    eval no
#+INFOJS_OPT:  view:showall toc:nil ltoc:nil mouse:underline path:http://thomasf.github.io/solarized-css/org-info.min.js
#+HTML_HEAD:   <link rel="stylesheet" type="text/css" href="http://thomasf.github.io/solarized-css/solarized-light.min.css" />
#+HTML_HEAD:   <base target="_blank">
#+BEGIN_abstract
This document presents an integrative framework to make predictions
from spatial autoregressive models (Cliff and Ord 1973, 1981; Anselin
1988). It also contains the corresponding [[url:http://www.r-project.org][=R= code]] to implement the
predictors presented. The code is gathered into the [[./sppred.R][=sppred=]] function
that implements in particular the predictors from LeSage and Pace
(2004, 2008) and Kelejian and Prucha (2004) for a large number of
specifications of spatial autocorrelation from the [[url:http://cran.r-project.org/web/packages/spdep/index.html][=spdep= package]]
(Bivand 2014). To use this code, save the file [[./sppred.R][=sppred=]] in your
working directory then submit =source("sppred.R")= to =R=. Some
examples on the =boston= data are also available at section XX. The
status of this work is actually under construction, comments are
welcome.\\

*=STILL TODO=* 
- Code the variances and confidence intervals of predictors
- Code the predictors for ex-sample predictions
- Code the predictors for =sphet= and =splm= objects
#+END_abstract
#+LaTeX: \clearpage
#+TOC: headlines 1
#+LaTeX: \clearpage

* Theoretical Framework
** Spatial Econometric Models

   The particularity of spatial econometric models is the structural
   interdependence between statistical units. Associated predictions
   not only depend on the variables of the target units but also on
   the neighboring units. Hence, the usual distinction between
   in-sample (IS) and out-of-sample (OS) predictions has firstly to be
   refined. It is not simply inside our outside the calibration sample
   but it has also some relations (or not) with it. The literature
   about predicting from spatial econometric models is not really
   unified (see XX for the most significant), and one can consider
   this note as an attempt, with the associated practical framework
   implemented in =R=.

   To present analytically the available predictors, we start with the
   more general spatial autoregressive mixed conditional (SMC)
   specification of the Cliff-Ord (1973, 1981) heteroscedasticity
   class of models with exogenous covariates,[fn:1]

\begin{align}
y & = \rho Wy+X\beta+ WX\theta+ u\nonumber\\
u & = \lambda W u+ \varepsilon \nonumber
\end{align}

   with $u\sim \mathbf{N}(0, \sigma^2\cdot I_N)$. The $y$ is a
   $N\times 1$ vector continuous outcome, $X$ is a $N\times K$ matrix
   of the $K$ covariates, and $W$ is a $N\times N$ spatial weight
   matrix (see Anselin 1988). We limit ourselves to a same weight
   matrix in the outcome and error equations, but formally nothing
   requires this restriction. The unknown parameters $\rho$, $\beta$,
   $\theta$, $\lambda$ and $\sigma$ have to be estimated, as the
   vector $\varepsilon$ of innovations. Classically, we assume that
   $\mbox{diag}(W)= 0$ and $|\,\rho\,|$, $|\,\lambda\,|< 1$ but the
   spatial weight matrix $W$ does not need to be symmetric.

   This form is sufficiently general that the spatial autoregressive
   conditional (SAC) model can be recovered with $\theta= 0$, the
   spatial Durbin model (SDM) model can be recovered when $\lambda=0$,
   the spatial autoregressive (SAR) model with $\theta=\lambda=0$, the
   spatial X model (SXM, also called Spatial Durban Error Model by
   LeSage and Pace, 2009) with $\rho=0$ and the spatial error model
   (SEM) can be recovered with $\rho=\theta= 0$. All these models can
   be estimated through the functions =errorsarlm=, =lagsarlm=, and
   =sacsarlm= of the =R= package =spdep= (Bivand, 2014).

   The geo-statistical models (Cressie 1993) usually involve
   specifying spatial dependence through the error process (like in
   SEM or SXM) as opposed to the spatial lag of the outcome vector
   (like in SAR or SDM). Predicting from these error models take a
   simpler form than autoregressive models and can be easily recovered
   from the theoretical framework presented here. The reverse is not
   true.

** Making Predictions

   Basically, . in terms of conditional expectation put the focus of
   the available information used to construct the predictors which is
   important for the transparency of the code. We consider $\beta$
   known to focus on prediction formula.

\begin{equation}
\tag{PRD.X}\mathbf{E}(y\mid X)= X \widehat{\beta}\nonumber
\end{equation}

\begin{equation}
\tag{PRD.WX}\mathbf{E}(y\mid X, W)= X\widehat{\beta}+ WX\widehat{\theta}
\end{equation}

\begin{equation}
\tag{PRD.KP1}\mathbf{E}(y\mid X, W)= (I_n-\rho W)^{-1} X\widehat{\beta}+ WX\widehat{\theta}
\end{equation}

   This correspond to KP1 but also the exogenous predictor of LSP. It
   is the default predictor of the function =sppred=.

\begin{equation}
\tag{PRD4}\mathbf{E}(y\mid X, W, Wy)= \rho Wy+ X\widehat{\beta}+ WX\widehat{\theta}+ \lambda (Wy- X\beta- WX\theta)
\end{equation}

   It is a bit strange but $y$ cannot be recovered from $W$ and
   $Wy$. KP analyses this predictor separately in KP4 and KP5 but such
   a distinction is not necessary.  Since $\mbox{diag}(W)=0$, $W y$
   does not use individually the $y_i$ to predict itself but needs
   them all to predict the entire vector.

   The bias of actual predictors come from the correlation between the
   spatially lagged dependent variable and the error term. It is why
   the use of best linear unbiased predictor (Golberger) is of
   particular importance.

\begin{equation}
\tag{PRD5}\mathbf{E}(y\mid X, W, y_{IS})= (I_n-\rho W)^{-1} X\widehat{\beta}+ WX\widehat{\theta}
                                        + \Omega_{21}\Omega_{11}^{-1}(y_{IS}-\mathbf{E}(y_{IS}\mid X, W))
\end{equation}

   The covariance can be simplified with precision matrix (Harville)
   which do not necessitate the inversion of matrix:
   $\Omega_{21}\Omega_{11}^{-1}= \Psi_{21}\Psi_{11}^{-1}$. LSP also
   present a leave-one-out (loo) specification of this predictor, that
   is based on the same conditioning set, except that we consider the
   predictions individually of each OS units indexed $i$.

\begin{equation}
\tag{prdLSP}\mathbf{E}(y_i\mid X, W, y_{IS})= (I_n-\rho W)^{-1}X\widehat{\beta}+ WX\widehat{\theta}
                                            + \Omega_{21}\Omega_{11}^{-1}(y_{IS}-\mathbf{E}(y_{IS}\mid X, W))\label{prdLSP}
\end{equation}

   This simplification comes from...

\begin{equation}
\tag{prdKP2}\mathbf{E}(y_i\mid X, W, y_{IS})= \rho Wy+ X\widehat{\beta}+ WX\widehat{\theta} 
                                            +  \Omega_{21}\Omega_{11}^{-1}(y_{IS}-\mathbf{E}(y_{IS}\mid X, W))
\end{equation}

     yopla

\begin{equation}
\tag{prdKP2}\mathbf{E}(y_i\mid X, W, y_{IS})= \rho Wy+ X\widehat{\beta}+ WX\widehat{\theta}
                                        + \Omega_{21}\Omega_{11}^{-1}(y_{IS}-\mathbf{E}(y_{IS}\mid X, W))
\end{equation}

   If we put aside the differences in the conditional mean of the
   exogenous part, this predictor =predKP2= is identical to [[ref:prdLSP][prdLSP]].


   Can we still maintain the signal trend distinction? Does it the
   same as direct and indirect effects of covariates?

   We develop a framework of prediction from models with
   interdependent observations.

   We implement the KP1 predictors, also called exogenous by LeSage
   and Pace.

   We have to explain the differences between in-sample, out-of-sample
   and ex-sample in a spatial context. Ex-sample is not necessary
   linked to temporal, it is also interesting to counterfactual
   simulations. The prediction in out-of-sample needs a certain
   spatial embedding between the two spatial samples, not having
   sampled neighbors does not mean no neighbors. But in a spatial
   segregative case, this corresponds to a ex-sample case.

* Current function from =spdep=

  Our code is an extension of the function =predict.sarlm()= actually
  the default function from the package =spdep= (Bivand).

#+Name: Lst:DFT
#+begin_src R :results output :file "predict-sarlm.R"
library(spdep) ; predict.sarlm
#+end_src

#+RESULTS: Lst:DFT
[[file:predict-sarlm.R]]

  The current function, accessible through previous link, implement
  different predictor according to the absence of the presence of
  newdata. For the in-sample predictions (=if(newdata=== =NULL)=), the
  predictors are computed as Eq. XX using BLUP. For the out of sample
  predictions (=if(newdata!== =NULL)=), the predictors are computed as
  Eq. XX using biased and inefficient predictors. It produces
  inconsistencies by not implementing the same predictions if we put
  the data that are used to fit the model in the =newdata= argument
  (cf. XX example below). Another shortcoming of the current function
  is the class of objects from SEM and SXM: they are not
  vectors. Lastly, if we put =sacmixed= objects in the current
  function, they are not recognized as such and produce some errors
  about matrix dimension.

  At the center of this distinction is the observability of the
  outcome variable $y$.

  Some other particularities are present in the current function. The
  OS predictor for error models is KP1 but not directly for lag
  models. For that, we have to put =legacy=== =FALSE=. The signal is
  computed by difference for the lag models in out of sample.

* The =sppred= extension
** General Structure

   Here is the general structure of the functions that call
   sub-functions that are defined below.

   This function contents the usual verifications, with 2 more
   arguments: =cond.set= for the conditional set (see XX) and =mean=
   for the specification of the structural mean.

   It is important that the same predictor is implemented when
   newdata are NULL or not, as when spatial matrix set.

   The scan for the lagged WX is by the presence of "lag." at their
   name, it has to be changed.

#+begin_src R :results silent :tangle ./sppred.R
sppred <- function(object, newdata = NULL, listw = NULL, yobs= object$y,
                   condset= "DEF", blup = NULL, loo = FALSE, power = NULL,
                   zero.policy = NULL, legacy = TRUE, order = 250,
                   tol= .Machine$double.eps^(3/5), ...) {
    require(spdep)
    ## USUAL VERIFICATIONS
    if (is.null(zero.policy)) 
        zero.policy <- get("zeroPolicy", envir = spdep:::.spdepOptions)
    stopifnot(is.logical(zero.policy))
    if (is.null(power)) power <- object$method != "eigen"
    stopifnot(is.logical(legacy)) ; stopifnot(is.logical(power))
    ## DETERMINING THE MODEL
    if (object$type== "error"){
        mod <- ifelse(object$etype== "error", "sem", "sxm")
    } else {
        mod <- switch(object$type, "lag"= "sar", "mixed"= "sdm",
                                   "sac"= "sac", "sacmixed"= "smc")
    }
    ## DATA SHAPING
    if (mod %in% c("sem", "sxm")) {lab= object$lambda ; rho= 0         }
    if (mod %in% c("sar", "sdm")) {lab= 0             ; rho= object$rho}
    if (mod %in% c("sac", "smc")) {lab= object$lambda ; rho= object$rho}
    Wlg <- substr(names(object$coefficients), 1, 4)== "lag."
    B <- object$coefficients[ !Wlg] ; Bl <- object$coefficients[ Wlg]
    if (is.null(newdata)){
        X   <- object$X[, !Wlg]
    } else {
        frm <- formula(object$call)
        mt  <- delete.response(terms(frm, data = newdata))
        mf  <- model.frame(mt, newdata)
        X   <- model.matrix(mt, mf)
        if (any(object$aliased)) X <- X[, -which(object$aliased)]
    }
    ## WEIGHT MATRIX, add an error message
    if (is.null(listw)) lsw <- eval(object$call$listw) else lsw <- listw
    ## PREDICTORS
    if (is.null(blup)){
        pt <- switch(condset, "X"= 1, "XW"= 2, "DEF"= 3, "XWy"= 4)
    } else {
        pt <- switch(blup, "LSP"= 5, "KP2"= 6, "KP3"= 7, "KPG"= 8)
    }
    prdX <- as.vector(X %*% B) ; print(pt)
    if (pt> 1) prdWX   <- prdWX(prdX, X, Bl, mod, lsw)
    if (pt> 2 && pt!= 4) prdKP1  <- prdKP1(prdWX, rho, lsw, power, order, tol)
    if (pt> 3){
        prdWXy <- prdWX+ rho* lag.listw(lsw, yobs)
                       + lab* lag.listw(lsw, yobs- prdWX)}
    if (pt==5) prdLSP <- prdLSP(prdKP1, rho, lab, lsw, yobs, loo)
    if (pt> 5 && !loo) stop("Set loo= TRUE for this blup predictor")
    if (pt==6){
        prdKP2 <- prdKP2(prdKP1, prdWXy,
                         rho, lab, lsw, yobs, power, order, tol)}
    if (pt==7){
        prdKP3 <- prdKP3(prdKP1, prdWXy,
                         rho, lab, lsw, yobs, power, order, tol)}
    if (pt==8) stop("not implemented")
    prd <- switch(pt, "1"= prdX, "2"= prdWX, "3"= prdKP1, "4"= prdWXy,
                      "5"= prdLSP, "6"= prdKP2, "7"= prdKP3, "8"= prdKPG)
    class(prd) <- "sppred" ; as.vector(prd)
}
#+end_src

   we choose to not use =object$tarX= and =object$tarY= for more
   transparencies. It is clear that we lost from that in terms of
   computation time. It is easy to predict by conditioning only on "X"
   because it is the same form for all the spatial models (see
   equation XX).

** Predictors conditioned on X, W
*** exogenous predictor

#+begin_src R :results silent :tangle ./sppred.R
prdWX <- function(prdX, X= X, Bl= Bl, mod= mod, lsw= lsw){
    if (!mod %in% c("sxm", "sdm", "smc")){
        prdWX <- prdX } else {
            K <- ifelse(colnames(X)[ 1] == "(Intercept)", 2, 1)
            m <- ncol(X) ; WX <- matrix(nrow= length(prdX), ncol= m+ 1- K)
            for (k in K: m){
                WX[, k+ 1- K] <- lag.listw(lsw, X[, k])
            }
            prdWX <- prdX+ (WX %*% Bl)
        } 
    prdWX
}
#+end_src

*** endogenous predictor

#+begin_src R :results silent :tangle ./sppred.R
prdKP1 <- function(prdWX, rho= rho, lsw= lsw,
                   power= power, order= order, tol= tol){
    if (power){
        W <- as(as_dgRMatrix_listw(lsw), "CsparseMatrix")
        prdKP1 <- c(as(powerWeights(W, rho= rho, X= as.matrix(prdWX),
                                    order= order, tol= tol), "matrix"))
    } else {
        prdKP1 <- c(invIrW(lsw, rho) %*% prdWX)
    }
    prdKP1
}
#+end_src

** Predictors conditioned on X, W, y
*** biased predictors

    The predictors equivalent to KP4 and KP5, we do not let the choice
    (because the omitted combination can be recovered from previous
    predictors) and we can eventually add a KP6 for SAC and SMC
    models. The computations are in the general.

*** BLUP LSP

    It can make sens to distinguish one shot to one leave one.


#+begin_src R :results silent :tangle ./sppred.R
prdLSP <- function(prdKP1, rho= rho, lab= lab,
                   lsw= lsw, yobs= yobs, loo= loo){
    ZL <- diag(length(prdKP1))- (lab* listw2mat(lsw))
    ZR <- diag(length(prdKP1))- (rho* listw2mat(lsw))
    Z  <- ZL %*% ZR ; P22 <- t(Z) %*% Z
    if (loo){
        prdLSP <- matrix(NA, ncol= 1, nrow= length(prdKP1))
        for (i in 1: length(prdKP1)){
            prdLSP[ i] <- prdKP1[ i]-
                (P22[-i, i] %*% (yobs[ -i]- prdKP1[ -i])/ P22[i, i])
        }
    } else {
        P11 <- P22
        prdLSP <- prdKP1+ ((solve(P22) %*% P11 %*% (yobs- prdKP1)))
    }
    prdLSP
}
#+end_src

*** BLUP KP2

#+begin_src R :results silent :tangle ./sppred.R
prdKP2 <- function(prdKP1, prdWXy= prdWXy, rho= rho, lab= lab, lsw= lsw,
                   yobs= yobs, power= power, order= order, tol= tol){
    if (power){
        W <- as(as_dgRMatrix_listw(lsw), "CsparseMatrix")
        GL <- as(powerWeights(W, rho= lab, order= order, tol= tol,
                              X= diag(length(prdWXy))), "matrix")
        GR <- as(powerWeights(W, rho= rho, order= order, tol= tol,
                              X= diag(length(prdWXy))), "matrix")
    } else {
        GL <- invIrW(lsw, rho) ; GR <- invIrW(lsw, lab)
    }
    sum.u <- GL %*% t(GL) ; sum.y <- GR %*% sum.u %*% t(GR)
    WM <- listw2mat(lsw)[i, ]
    prdKP2 <- matrix(NA, ncol= 1, nrow= length(prdWXy))
    for (i in 1: length(prdKP2)){
        rg <- (sum.u[i, ] %*% GR %*% WM)/ (WM %*% sum.y %*% WM)
        prdKP2[ i] <- prdWXy[ i]+ (rg %*% WM %*% (yobs- prdKP1))
    }
    prdKP2
}
#+end_src

*** BLUP KP3

#+begin_src R :results silent :tangle ./sppred.R
prdKP3 <- function(prdKP1, prdWXy= prdWXy, rho= rho, lab= lab, lsw= lsw,
                   yobs= yobs, power= power, order= order, tol= tol){
    if (power){
        W <- as(as_dgRMatrix_listw(lsw), "CsparseMatrix")
        GL <- as(powerWeights(W, rho= lab, order= order, tol= tol,
                              X= diag(length(prdWXy))), "matrix")
        GR <- as(powerWeights(W, rho= rho, order= order, tol= tol,
                              X= diag(length(prdWXy))), "matrix")
    } else {
        GL <- invIrW(lsw, lab) ; GR <- invIrW(lsw, rho)
    }
    sum.u <- GL %*% t(GL) ; sum.y <- GR %*% sum.u %*% t(GR)
    prdKP3 <- matrix(NA, ncol= 1, nrow= length(prdWXy))    
    for (i in 1: length(prdKP3)){
        rg <- sum.u[i, ] %*% GR[, -i] %*% solve(sum.y[-i, -i])
        prdKP3[ i] <- prdWXy[ i]+ (rg %*% (yobs[-i ]- prdKP1[-i ]))
    }
    prdKP3
}
#+end_src

** Predictors conditioned by hand

   Try with

#+BEGIN_QUOTE
        rg <- sum.u[i, ] %*% GR %*% solve(sum.y)
        prdKP3[ i] <- prdWXy[ i]+ (rg %*% (yobs- prdKP1))
#+END_QUOTE

* How it works
** Choosing a type of predictor

   Our new =R= function for spatial predictions -- called =sppred= for
   the moment -- admits a first additional argument =predictor= that
   specify the computed predictor. Knowing that predictors
   corresponding to larger information sets are more complex,
   flexibility is needed to let the user makes its own trade-off
   between simplicity and prediction efficiency. The following table
   define the available predictors.

#+Caption: The available values for the new =predictor= argument
|-------------+---------------------+-------------------|
| =predictor= | label               | equation (see XX) |
|-------------+---------------------+-------------------|
| "1"         | minimum information | (XX)              |
| "2"         | heuristic BLUP      | (XX)              |
| "3"         | BLUP                | (XX)              |
| "4"         | heuristic data      | (XX)              |
|-------------+---------------------+-------------------|

   The =predictor= 4 is currently the default for IS prediction in
   =predict.sarlm= (it corresponds to the predictor KP4 for lag models
   and KP5 for error models).

** Specifying
** General structure, usual checks, and IS predictions

   Here the code, for the inverse integrating directly the code from
   powerWeigths?

** The predictors 1 for OS predictions   
* The =boston= example
** Estimating the models

#+begin_src R :results output exemple
library(spdep) ; data(boston)
#+end_src

** Predicting
* Testing							   :noexport:
** Sample

#+Name: Lst:PSS
#+Header: :width 11 :height 4
#+begin_src R :results graphics :file "Figures/PrsSpSmp.pdf"
load("Data/exsmp.Rda") ; library(spdep)
plot(exsmp$Dat.all)
plot(exsmp$Dat.cal, col= "blue", pch= 20, add= TRUE)
#+end_src

#+Name: Fig:PSS
#+ATTR_LaTeX: :options scale= .5
#+Caption: Calibration and exhaustive datasets
#+RESULTS: Lst:PSS
[[file:Figures/PrsSpSmp.pdf]]
 
** Estimating the spatial models

#+begin_src R :results output exemple
SEM <- errorsarlm(ARlog03~ PXLB03+ RTFO03+ BdAlti, data= exsmp$Dat.cal,
                  exsmp$Wgt.cal, method= "eigen")
SXM <- errorsarlm(ARlog03~ PXLB03+ RTFO03+ BdAlti, data= exsmp$Dat.cal,
                  exsmp$Wgt.cal, method= "eigen", etype= "emixed")
SAR <- lagsarlm(  ARlog03~ PXLB03+ RTFO03+ BdAlti, data= exsmp$Dat.cal,
                  exsmp$Wgt.cal, method= "eigen")
SDM <- lagsarlm(  ARlog03~ PXLB03+ RTFO03+ BdAlti, data= exsmp$Dat.cal,
                  exsmp$Wgt.cal, method= "eigen", type= "mixed")
SAC <- sacsarlm(  ARlog03~ PXLB03+ RTFO03+ BdAlti, data= exsmp$Dat.cal,
                  exsmp$Wgt.cal, method= "eigen")
SMC <- sacsarlm(  ARlog03~ PXLB03+ RTFO03+ BdAlti, data= exsmp$Dat.cal,
                  exsmp$Wgt.cal, method= "eigen", type= "sacmixed")
library(plyr)
t(ldply(list(SEM, SXM, SAR, SDM, SAC, SMC), AIC))
#+end_src

#+RESULTS:
:        [,1]     [,2]     [,3]     [,4]     [,5]    [,6]
: V1 445.7127 433.3333 435.5886 434.1438 436.3016 435.197

** Testing the predictors
*** Conditioned on X

#+begin_src R :results output exemple
source("sppred.R")
SEMprdX <- sppred(SEM, condset= "X")
sqrt(mean(I(SEMprdX- SEM$y)^2))
SXMprdX <- sppred(SXM, condset= "X")
sqrt(mean(I(SXMprdX- SXM$y)^2))
SARprdX <- sppred(SAR, condset= "X")
sqrt(mean(I(SARprdX- SAR$y)^2))
SDMprdX <- sppred(SDM, condset= "X")
sqrt(mean(I(SDMprdX- SDM$y)^2))
SACprdX <- sppred(SAC, condset= "X")
sqrt(mean(I(SACprdX- SAC$y)^2))
SMCprdX <- sppred(SMC, condset= "X")
sqrt(mean(I(SMCprdX- SMC$y)^2))
SMCprdX <- sppred(SMC, newdata= exsmp$Dat.cal, condset= "X", power= T)
sqrt(mean(I(SMCprdX- SMC$y)^2))

## A ESSAYER AVEC L'AUTRE BASE SPATIALE
#+end_src

*** Conditioned on X, W

#+begin_src R :results output exemple
source("sppred.R")
SEMprdWX <- sppred(SEM, condset= "XW")
sqrt(mean(I(SEMprdWX- SEM$y)^2))
SXMprdWX <- sppred(SXM, condset= "XW")
sqrt(mean(I(SXMprdWX- SXM$y)^2))
SARprdWX <- sppred(SAR, condset= "XW")
sqrt(mean(I(SARprdWX- SAR$y)^2))
SDMprdWX <- sppred(SDM, condset= "XW")
sqrt(mean(I(SDMprdWX- SAR$y)^2))
SACprdWX <- sppred(SAC, condset= "XW")
sqrt(mean(I(SACprdWX- SAR$y)^2))
SMCprdWX <- sppred(SMC, condset= "XW")
sqrt(mean(I(SMCprdWX- SAR$y)^2))
SXMprdXW <- sppred(SXM, newdata= exsmp$Dat.cal,
                   condset= "XW", listw= exsmp$Wgt.cal)
sqrt(mean(I(SXMprdWX- SXM$y)^2))
#+end_src

*** KP1 predictors

#+begin_src R :results output exemple
source("sppred.R")
SEMprdKP1 <- sppred(SDM, power= TRUE)
sqrt(mean(I(SEMprdKP1- SEM$y)^2))
SXMprdKP1 <- sppred(SXM)
sqrt(mean(I(SXMprdKP1- SXM$y)^2))
SARprdKP1 <- sppred(SAR)
sqrt(mean(I(SARprdKP1- SAR$y)^2))
SDMprdKP1 <- sppred(SDM)
sqrt(mean(I(SDMprdKP1- SAR$y)^2))
SACprdKP1 <- sppred(SAC)
sqrt(mean(I(SACprdKP1- SAR$y)^2))
SMCprdKP1 <- sppred(SMC)
sqrt(mean(I(SMCprdKP1- SAR$y)^2))

yop <- sppred(SXM, condset= "XW")
WWW <- as(as_dgRMatrix_listw(exsmp$Wgt.cal), "CsparseMatrix")
pp <- c(as(powerWeights(WWW, rho= 0, X= as.matrix(yop)), "matrix"))

SXMprdKP1 <- sppred(SXM, newdata= exsmp$Dat.cal)
sqrt(mean(I(SXMprdKP1- SXM$y)^2))
SXMprdKP1 <- sppred(SXM, power= TRUE)
SXMprdKP1 <- sppred(SXM, newdata= exsmp$Dat.cal, listw= exsmp$Wgt.cal)
SXMprdKP1 <- sppred(SXM, newdata= exsmp$Dat.cal, listw= exsmp$Wgt.cal,
                    power= TRUE)
sqrt(mean(I(SXMprdKP1- SXM$y)^2))

#+end_src

*** Biased predictor

#+begin_src R :results output exemple
source("sppred.R")
SEMprdXWy <- sppred(SEM, condset= "XWy")
sqrt(mean(I(SEMprdXWy- SEM$y)^2))
SXMprdXWy <- sppred(SXM, condset= "XWy")
sqrt(mean(I(SXMprdXWy- SXM$y)^2))
SARprdXWy <- sppred(SAR, condset= "XWy")
sqrt(mean(I(SARprdXWy- SAR$y)^2))
SDMprdXWy <- sppred(SDM, condset= "XWy")
sqrt(mean(I(SDMprdXWy- SDM$y)^2))
SACprdXWy <- sppred(SAC, condset= "XWy")
sqrt(mean(I(SACprdXWy- SAC$y)^2))
SMCprdXWy <- sppred(SMC, condset= "XWy")
sqrt(mean(I(SMCprdXWy- SMC$y)^2))

SDMprdX <- sppred(SDM, condset= "XWy", power= TRUE)
SDMprdX <- sppred(SDM, condset= "XWy", newdata= exsmp$Dat.cal)
sqrt(mean(I(SDMprdXWy- SDM$y)^2))

## ON PEUT COMMENCER A METTRE DU yobs choisit
#+end_src

*** LSP predictor

#+begin_src R :results output exemple
source("sppred.R")
SEMprdKP1 <- sppred(SEM, blup= "LSP")
sqrt(mean(I(SEMprdKP1- SEM$y)^2))

SEMprdKP1 <- sppred(SEM, blup= "LSP", loo= TRUE)
sqrt(mean(I(SEMprdKP1- SEM$y)^2))


SXMprdXWy <- sppred(SXM, blup= "LSP", loo= TRUE)
sqrt(mean(I(SXMprdXWy- SXM$y)^2))

SARprdXWy <- sppred(SAR, condset= "XWy")
sqrt(mean(I(SARprdXWy- SAR$y)^2))
SDMprdXWy <- sppred(SDM, condset= "XWy")
sqrt(mean(I(SDMprdXWy- SDM$y)^2))
SACprdXWy <- sppred(SAC, condset= "XWy")
sqrt(mean(I(SACprdXWy- SAC$y)^2))
SMCprdXWy <- sppred(SMC, condset= "XWy")
sqrt(mean(I(SMCprdXWy- SMC$y)^2))

SDMprdX <- sppred(SDM, condset= "XWy", power= TRUE)
SDMprdX <- sppred(SDM, condset= "XWy", newdata= exsmp$Dat.cal)
sqrt(mean(I(SDMprdXWy- SDM$y)^2))

## ON PEUT COMMENCER A METTRE DU yobs choisit
#+end_src

*** KP2 predictor

#+begin_src R :results output exemple
source("sppred.R")
SEMprdKP2 <- sppred(SEM, blup= "KP2", loo= TRUE)
sqrt(mean(I(SEMprdKP2- SEM$y)^2))
SXMprdKP2 <- sppred(SXM, blup= "KP2", loo= TRUE)
sqrt(mean(I(SXMprdKP2- SXM$y)^2))
SARprdKP2 <- sppred(SAR, blup= "KP2", loo= TRUE)
sqrt(mean(I(SARprdKP2- SAR$y)^2))
SDMprdKP2 <- sppred(SDM, blup= "KP2", loo= TRUE)
sqrt(mean(I(SDMprdKP2- SDM$y)^2))
SACprdKP2 <- sppred(SAC, blup= "KP2", loo= TRUE)
sqrt(mean(I(SACprdKP2- SAC$y)^2))
SMCprdKP2 <- sppred(SMC, blup= "KP2", loo= TRUE)
sqrt(mean(I(SMCprdKP2- SMC$y)^2))

SMCprdKP2 <- sppred(SMC, condset= "X", blup= "KP2", loo= TRUE)
sqrt(mean(I(SMCprdKP2- SMC$y)^2))

## ON PEUT COMMENCER A METTRE DU yobs choisit
#+end_src

*** KP3 predictor

#+begin_src R :results output exemple
source("sppred.R")
SEMprdKP3 <- sppred(SEM, blup= "KP3", loo= TRUE)
sqrt(mean(I(SEMprdKP3- SEM$y)^2))
yop <- sppred(SEM, blup= "LSP", loo= TRUE)
sqrt(mean(I(yop- SEM$y)^2))

system.time(SXMprdKP3 <- sppred(SXM, blup= "KP3", loo= TRUE, power= FALSE))
sqrt(mean(I(SXMprdKP3- SXM$y)^2))
system.time(yop <- sppred(SXM, blup= "LSP", loo= TRUE, power= TRUE))
sqrt(mean(I(yop- SXM$y)^2))


system.time(SARprdKP3 <- sppred(SAR, blup= "KP3", loo= TRUE))
sqrt(mean(I(SARprdKP3- SAR$y)^2))
system.time(yop <- sppred(SAR, blup= "LSP", loo= TRUE, power= TRUE))
sqrt(mean(I(yop- SAR$y)^2))

a <- SARprdKP3- sppred(SAR, condset= "XWy")
b <- yop- sppred(SAR)
plot(a, b)


SDMprdKP3 <- sppred(SDM, blup= "KP3", loo= TRUE)
sqrt(mean(I(SDMprdKP3- SDM$y)^2))
SACprdKP3 <- sppred(SAC, blup= "KP3", loo= TRUE)
sqrt(mean(I(SACprdKP3- SAC$y)^2))
SMCprdKP3 <- sppred(SMC, blup= "KP3", loo= TRUE, power= TRUE)
sqrt(mean(I(SMCprdKP3- SMC$y)^2))
yop <- sppred(SMC, blup= "LSP", loo= TRUE)
sqrt(mean(I(yop- SMC$y)^2))




SMCprdKP3 <- sppred(SMC, condset= "X",
                    newdata= exsmp$Dat.cal[ 1: 10,],
                    blup= "KP3", loo= TRUE)
sqrt(mean(I(SMCprdKP3- SMC$y)^2))

yop <- sppred(SMC, blup= "LSP", loo= TRUE)
sqrt(mean(I(SMCprdKP3- SMC$y)^2))

plot(yop, SMCprdKP3)

## ON PEUT COMMENCER A METTRE DU yobs choisit
#+end_src

* Summary
** Changes relative to =predict.sarlm=

   - Implement predictions for SARAR and Mixed SARAR models from
     respectively =sac= and =sacmixed= classes.
   - Compute BLUP and almost BLUP spatial predictors
   - About the in-sample / out of sample structure (=newdata=)
   - About the distinction between trend and signal
   - The simplification of the in-sample predictions

** About the intercept

   We change the scan of the intercept, in particular in presence of
   $WX$ in the regression. If $W$ is row standardized, we have to drop
   the intercept to avoid collinearity. The initial function add the
   constant at the end of the computations, we only drop the intercept
   in the presence of $WX$.

* Footnotes

[fn:1] This terminology comes from LeSage and Pace (2009) and Bivand
(2014). This model is called spatial autoregressive model with
autoregressive disturbances, SARAR(1,1), by Kelejian and Prucha
(1998). The notations are also matter of discussion, we adopt those of
the =R= package =spdep= (Bivand, 2014).

