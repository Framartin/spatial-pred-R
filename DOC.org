#+TITLE:       Extending Predictions from Spatial Econometric Models on R
#+AUTHOR:      Jean-Sauveur AY \\ \lt[[mailto:jsay.site@gmail.com][jsay.site@gmail.com]]\gt \and Julie LE GALLO\\ \lt[[mailto:jlegallo@univ-fcomte.fr][jlegallo@univ-fcomte.fr]]\gt 
#+LaTeX_CLASS: ManueStat
#+OPTIONS:     LaTeX:t tags:nil toc:nil H:5
#+STARTUP:     hideblocks
#+BIND:        org-latex-image-default-width ""
#+BIND:        org-latex-tables-booktabs t
#+PROPERTY:    session *R*
#+PROPERTY:    exports both
#+PROPERTY:    eval no
#+INFOJS_OPT:  view:t toc:t mouse:underline buttons:0 path:http://thomasf.github.io/solarized-css/org-info.min.js
#+HTML_HEAD:   <link rel="stylesheet" type="text/css" href="http://thomasf.github.io/solarized-css/solarized-light.min.css" />
#+BEGIN_abstract
This document presents a framework and some =R= code --
[[latex:url][www.r-project.org]] -- to make predictions from spatial autoregressive
models. In particular, it implements the predictors from LeSage and
Pace (2004, 2008) and Kelejian and Prucha (2004) for a large number of
autoregressive models from the =spdep= package (Bivand 2014). To use
this code, save the file =sppred.R= in your working directory then
enter: =source("sppred.R")=. The status is actually under
construction, comments are welcome.\\

*TODO*
- Code the variances and confidence intervals of predictors
- Allow different weight matrix between lags and errors
- Code the predictors for =sphet= and =splm= objects
#+END_abstract
#+BEGIN_LaTeX
  \clearpage
  \setcounter{tocdepth}{2}
  \begin{spacing}{0.75}
  \tableofcontents
  \end{spacing}
  \clearpage
#+END_LaTeX

# http://www.springerreference.com/docs/html/chapterdbid/62922.html

* Major changes relative to =predict.sarlm=

   - Implement predictions for SARAR and Mixed SARAR models from
     respectively =sac= and =sacmixed= classes.
   - Compute BLUP and almost BLUP spatial predictors
   - About the in-sample / out of sample structure (=newdata=)
   - About the distinction between trend and signal
   - The simplification of the in-sample predictions

** About the intercept

   We change the scan of the intercept, in particular in presence of
   $WX$ in the regression. If $W$ is row standardized, we have to drop
   the intercept to avoid collinearity. The initial function add the
   constant at the end of the computations, we only drop the intercept
   in the presence of $WX$.

* Theoretical Framework
** Spatial Econometric

   When considering predicting from spatial econometric models, the
   first point is to recognize that observations are structurally
   interdependent and neighboring observation can improve the quality
   of predictions. Hence, the usual distinction between in-sample (IS)
   and out-of-sample (OS) predictions has firstly to be refined. It is
   not simply inside our outside the calibration sample but it has
   also some relations (or not) with it. The literature about
   predicting from spatial econometric models is not really unified
   (see XX for the most significant), and one can consider this note
   as an attempt, with the associated practical framework implemented
   in =R=.

   From the more general from of the Cliff-Ord (1973, 1981)
   homoscedastic class of models with exogenous covariates,[fn:1]

\begin{align}
y           & = \rho Wy+X\beta+\gamma WX+ \varepsilon\nonumber\\
\varepsilon & = \lambda W\varepsilon+ u \nonumber
\end{align}

   with $u\sim \mathbf{N}(0, \sigma^2\cdot I_N)$. The $y$ is a
   $N\times 1$ vector continuous outcome, $X$ is a $N\times K$ matrix
   of the $K$ covariates, and $W$ is a $N\times N$ spatial weight
   matrix. We limit ourselves to a same weight matrix in the outcome
   and error equations, but nothing precludes this restriction. The
   unknown parameters $\rho$, $\gamma$, $\lambda$ and $\sigma$ have to
   be estimated, as the vector $u$ of residuals. Classically, we
   assume that $\mbox{diag}(W)= 0$, $\mid \rho \mid< 1$, $\mid \lambda
   \mid< 1$. /standardization of W?/ Not the same notations than
   KP 2007.

   By construction, $W_{ii}$ \gt $0$ to preclude an observation from
   directly predicting itself

   The geo-statistical models usually involve specifying spatial
   dependence through the error process as opposed to the spatial lag
   of the y vector, and these "error models" take a simpler form than
   autoregressive models

   This model is sufficiently general that the SARAR(1,1) model can be
   recovered with $\theta= 0$ (Kelejian2007) (also called SAC by
   Biva02, BPGR13), the spatial error model (SEM) can be recovered
   with $\rho=\theta= 0$, the spatial X model (SXM) with $\rho=0$, the
   spatial autoregressive (SAR) model with $\theta=\lambda=0$; and the
   spatial Durbin model (SDM) model can be recovered when
   $\lambda=0$. Another useful non exclusive distinction is the error
   models (SEM, SDM and SARAR) and the lag models (SAR, SDM and SARAR)

** Making Predictions

   The bias of actual predictors come from the correlation between
   the spatially lagged dependent variable and the error term.

   Since $W_{ii}=0$, $W y$ does not use $y_i$ to predict itself.

   Can we still maintain the signal trend distinction? Does it the
   same as direct and indirect effects of covariates?

   We develop a framework of prediction from models with
   interdependent observations.

   We implement the KP1 predictors, also called exogenous by LeSage
   and Pace.

   We have to explain the differences between in-sample, out-of-sample
   and ex-sample in a spatial context. Ex-sample is not necessary
   linked to temporal, it is also interesting to counterfactual
   simulations. The prediction in out-of-sample needs a certain
   spatial embedding between the two spatial samples, not having
   sampled neighbors does not mean no neighbors. But in a spatial
   segregative case, this corresponds to a ex-sample case.

* Current function from =spdep=

  Our code is an extension of the function =predict.sarlm()= actually
  the default function from the package =spdep= (Bivand).

#+Name: Lst:DFT
#+begin_src R :results output :file "predict-sarlm.R"
library(spdep) ; predict.sarlm
#+end_src

#+RESULTS: Lst:DFT
[[file:predict-sarlm.R]]

  The current function, accessible through previous link, implement
  different predictor according to the absence of the presence of
  newdata. For the in-sample predictions (=if(newdata=== =NULL)=), the
  predictors are computed as Eq. XX using BLUP. For the out of sample
  predictions (=if(newdata!== =NULL)=), the predictors are computed as
  Eq. XX using biased and inefficient predictors. It produces
  inconsistencies by not implementing the same predictions if we put
  the data that are used to fit the model in the =newdata= argument
  (cf. XX example below). Another shortcoming of the current function
  is the class of objects from SEM and SXM: they are not
  vectors. Lastly, if we put =sacmixed= objects in the current
  function, they are not recognized as such and produce some errors
  about matrix dimension.

  At the center of this distinction is the observability of the
  outcome variable $y$.

  Some other particularities are present in the current function. The
  OS predictor for error models is KP1 but not directly for lag
  models. For that, we have to put =legacy=== =FALSE=. The signal is
  computed by difference for the lag models in out of sample.

* The =sppred= extension
** General Structure

   Here is the general structure of the functions that call
   sub-functions that are defined below.

   This function contents the usual verifications, with 2 more
   arguments: =cond.set= for the conditional set (see XX) and =mean=
   for the specification of the structural mean.

   It is important that the same predictor is implemented when
   newdata are NULL or not, as when spatial matrix set.

   The scan for the lagged WX is by the presence of "lag." at their
   name, it has to be changed.

#+begin_src R :results silent :tangle ./sppred.R
sppred <- function(object, newdata = NULL, listw = NULL, yobs= object$y,
                   condset= "DEF", blup = NULL, loo = FALSE, power = NULL,
                   zero.policy = NULL, legacy = TRUE, order = 250,
                   tol= .Machine$double.eps^(3/5), ...) {
    require(spdep)
    ## USUAL VERIFICATIONS
    if (is.null(zero.policy)) 
        zero.policy <- get("zeroPolicy", envir = spdep:::.spdepOptions)
    stopifnot(is.logical(zero.policy))
    if (is.null(power)) power <- object$method != "eigen"
    stopifnot(is.logical(legacy)) ; stopifnot(is.logical(power))
    ## DETERMINING THE MODEL
    if (object$type== "error"){
        mod <- ifelse(object$etype== "error", "sem", "sxm")
    } else {
        mod <- switch(object$type, "lag"= "sar", "mixed"= "sdm",
                                   "sac"= "sac", "sacmixed"= "smc")
    }
    ## DATA SHAPING
    Wlg <- substr(names(object$coefficients), 1, 4)== "lag."
    B <- object$coefficients[ !Wlg]
    if (mod %in% c("sem", "sxm")) {lab= object$lambda ; rho= 0         }
    if (mod %in% c("sar", "sdm")) {lab= 0             ; rho= object$rho}
    if (mod %in% c("sac", "smc")) {lab= object$lambda ; rho= object$rho}
    if (is.null(newdata)){
        X   <- object$X[, !Wlg]
    } else {
        frm <- formula(object$call)
        mt  <- delete.response(terms(frm, data = newdata))
        mf  <- model.frame(mt, newdata)
        X   <- model.matrix(mt, mf)
        if (any(object$aliased)) X <- X[, -which(object$aliased)]
    }
    ## WEIGHT MATRIX, add an error message
    if (is.null(listw)) lsw <- eval(object$call$listw) else lsw <- listw
    ## PREDICTORS
    if (is.null(blup)){
        pt <- switch(condset, "X"= 1, "XW"= 2, "DEF"= 3, "XWy"= 4)
    } else {
        pt <- switch(blup, "LSP"= 5, "KP2"= 6, "KP3"= 7, "KPG"= 8)
    }
    prdX <- as.vector(X %*% B) ; print(pt)
    if (pt> 1) prdWX   <- prdWX(object, X, prdX, mod, lsw)
    if (pt> 2) prdKP1  <- prdKP1(prdWX, rho, lsw, power, order, tol)
    if (pt> 3 && pt!= 4){
        prdWXy <- prdWX+
            rho* lag.listw(lsw, yobs)+ lab* lag.listw(lsw, yobs- prdWX)}
    if (pt==5) prdLSP <- prdLSP(prdKP1, rho, lab, lsw, yobs, loo)
    if (pt> 5 && !loo) stop("Set loo= TRUE for this blup predictor")
    if (pt==6){
        prdKP2 <- prdKP2(prdKP1, prdWXy,
                         rho, lab, lsw, yobs, power, order, tol)}
    if (pt==7) prdKP3 <- prdKP3(object, prd, mod, B, X, lsw, yobs)
    if (pt==8) prdKPG <- prdKPG(mod, lsw, power, legacy, order, tol)
    prd <- switch(pt, "1"= prdX, "2"= prdWX, "3"= prdKP1, "4"= prdWXy,
                      "5"= prdLSP, "6"= prdKP2, "7"= prdKP3, "8"= prdKPG)
    class(prd) <- "sppred" ; as.vector(prd)
}
#+end_src

   we choose to not use =object$tarX= and =object$tarY= for more
   transparencies. It is clear that we lost from that in terms of
   computation time. It is easy to predict by conditioning only on "X"
   because it is the same form for all the spatial models (see
   equation XX).

** Predictors conditioned on X, W
*** exogenous predictor

#+begin_src R :results silent :tangle ./sppred.R
prdWX <- function(object, X= X, prdX= prdX, mod= mod, lsw= lsw){
    if (mod %in% c("sxm", "sdm", "smc")){
        K <- ifelse(colnames(X)[ 1] == "(Intercept)", 2, 1)
        m <- ncol(X) ; WX <- matrix(nrow= length(prdX), ncol= m+ 1- K)
        for (k in K: m){
            WX[, k+ 1- K] <- lag.listw(lsw, X[, k])
        }
        prdWX <- prdX+ (WX %*% object$coefficients[ m+ 1+ 0: (m- K)])
    } else prdWX <- prdX
   prdWX
}
#+end_src

*** endogenous predictor

#+begin_src R :results silent :tangle ./sppred.R
prdKP1 <- function(prdWX, rho= rho, lsw= lsw,
                   power= power, order= order, tol= tol){
    if (power){
        W <- as(as_dgRMatrix_listw(lsw), "CsparseMatrix")
        prdKP1 <- c(as(powerWeights(W, rho= rho, X= prdWX,
                                    order= order, tol= tol), "matrix"))
    } else {
        prdKP1 <- c(invIrW(lsw, rho) %*% prdWX)
    }
    prdKP1
}
#+end_src

** Predictors conditioned on X, W, y
*** biased predictors

    The predictors equivalent to KP4 and KP5, we do not let the choice
    (because the omitted combination can be recovered from previous
    predictors) and we can eventually add a KP6 for SAC and SMC
    models. The computations are in the general.

*** BLUP LSP

    It can make sens to distinguish one shot to one leave one.


#+begin_src R :results silent :tangle ./sppred.R
prdLSP <- function(prdKP1, rho= rho, lab= lab,
                   lsw= lsw, yobs= yobs, loo= loo){
    ZL <- diag(length(prdKP1))- (lab* listw2mat(lsw))
    ZR <- diag(length(prdKP1))- (rho* listw2mat(lsw))
    Z  <- ZL %*% ZR ; P22 <- t(Z) %*% Z
    if (loo){
        prdLSP <- matrix(NA, ncol= 1, nrow= length(prdKP1))
        for (i in 1: length(prdKP1)){
            prdLSP[ i] <- prdKP1[ i]-
                (P22[-i, i] %*% (yobs[ -i]- prdKP1[ -i])/ P22[i, i])
        }
    } else {
        P11 <- P22
        prdLSP <- prdKP1+ ((solve(P22) %*% P11 %*% (yobs- prdKP1)))
    }
    prdLSP
}
#+end_src

*** BLUP KP2

#+begin_src R :results silent :tangle ./sppred.R
prdKP2 <- function(prdKP1, prdWXy= prdWXy, rho= rho, lab= lab, lsw= lsw,
                   yobs= yobs, power= power, order= order, tol= tol){
    if (power){
        W <- as(as_dgRMatrix_listw(lsw), "CsparseMatrix")
        GL <- as(powerWeights(W, rho= lab, order= order, tol= tol,
                              X= diag(length(prdWXy))), "matrix")
        GR <- as(powerWeights(W, rho= rho, order= order, tol= tol,
                              X= diag(length(prdWXy))), "matrix")
    } else {
        GL <- invIrW(lsw, rho) ; GR <- invIrW(lsw, lab)
    }
    sum.u <- GL %*% t(GL) ; sum.y <- GR %*% sum.u %*% t(GR)
    WM <- listw2mat(lsw)
    prdKP2 <- matrix(NA, ncol= 1, nrow= length(prdWXy))
    for (i in 1: length(prdKP2)){
        rg <- (sum.u[i, ] %*% GR %*% WM[i, ])/ (WM[i, ] %*% sum.y %*% WM[i, ])
        prdKP2[ i] <- prdWXy[ i]+ (rg %*% WM[i, ] %*% (yobs- prdKP1))
    }
    prdKP2
}
#+end_src

*** BLUP KP3

#+begin_src R :results silent :tangle ./sppred.R
prdKP3 <- function(prdKP1, prdWXy= prdWXy, rho= rho, lab= lab, lsw= lsw,
                   yobs= yobs, power= power, order= order, tol= tol){
    prdKP3
}
#+end_src


** Predictors conditioned by hand
* How it works
** Choosing a type of predictor

   Our new =R= function for spatial predictions -- called =sppred= for
   the moment -- admits a first additional argument =predictor= that
   specify the computed predictor. Knowing that predictors
   corresponding to larger information sets are more complex,
   flexibility is needed to let the user makes its own trade-off
   between simplicity and prediction efficiency. The following table
   define the available predictors.

#+Caption: The available values for the new =predictor= argument
|-------------+---------------------+-------------------|
| =predictor= | label               | equation (see XX) |
|-------------+---------------------+-------------------|
| "1"         | minimum information | (XX)              |
| "2"         | heuristic BLUP      | (XX)              |
| "3"         | BLUP                | (XX)              |
| "4"         | heuristic data      | (XX)              |
|-------------+---------------------+-------------------|

   The =predictor= 4 is currently the default for IS prediction in
   =predict.sarlm= (it corresponds to the predictor KP4 for lag models
   and KP5 for error models).

** Specifying
** General structure, usual checks, and IS predictions

   Here the code, for the inverse integrating directly the code from
   powerWeigths?

** The predictors 1 for OS predictions   
* Testing
** Sample

#+Name: Lst:PSS
#+Header: :width 11 :height 4
#+begin_src R :results graphics :file "Figures/PrsSpSmp.pdf"
load("Data/exsmp.Rda") ; library(spdep)
plot(exsmp$Dat.all)
plot(exsmp$Dat.cal, col= "blue", pch= 20, add= TRUE)
#+end_src

#+Name: Fig:PSS
#+ATTR_LaTeX: :options scale= .5
#+Caption: Calibration and exhaustive datasets
#+RESULTS: Lst:PSS
[[file:Figures/PrsSpSmp.pdf]]
 
** Estimating the spatial models

#+begin_src R :results output exemple
SEM <- errorsarlm(ARlog03~ PXLB03+ RTFO03+ BdAlti, data= exsmp$Dat.cal,
                  exsmp$Wgt.cal, method= "eigen")
SXM <- errorsarlm(ARlog03~ PXLB03+ RTFO03+ BdAlti, data= exsmp$Dat.cal,
                  exsmp$Wgt.cal, method= "eigen", etype= "emixed")
SAR <- lagsarlm(  ARlog03~ PXLB03+ RTFO03+ BdAlti, data= exsmp$Dat.cal,
                  exsmp$Wgt.cal, method= "eigen")
SDM <- lagsarlm(  ARlog03~ PXLB03+ RTFO03+ BdAlti, data= exsmp$Dat.cal,
                  exsmp$Wgt.cal, method= "eigen", type= "mixed")
SAC <- sacsarlm(  ARlog03~ PXLB03+ RTFO03+ BdAlti, data= exsmp$Dat.cal,
                  exsmp$Wgt.cal, method= "eigen")
SMC <- sacsarlm(  ARlog03~ PXLB03+ RTFO03+ BdAlti, data= exsmp$Dat.cal,
                  exsmp$Wgt.cal, method= "eigen", type= "sacmixed")
library(plyr)
t(ldply(list(SEM, SXM, SAR, SDM, SAC, SMC), AIC))
#+end_src

#+RESULTS:
:        [,1]     [,2]     [,3]     [,4]     [,5]    [,6]
: V1 445.7127 433.3333 435.5886 434.1438 436.3016 435.197

** Testing the predictors
*** Conditioned on X

#+begin_src R :results output exemple
source("sppred.R")
SEMprdX <- sppred(SEM, condset= "X")
sqrt(mean(I(SEMprdX- SEM$y)^2))
SXMprdX <- sppred(SXM, condset= "X")
sqrt(mean(I(SXMprdX- SXM$y)^2))
SARprdX <- sppred(SAR, condset= "X")
sqrt(mean(I(SARprdX- SAR$y)^2))
SDMprdX <- sppred(SDM, condset= "X")
sqrt(mean(I(SDMprdX- SDM$y)^2))
SACprdX <- sppred(SAC, condset= "X")
sqrt(mean(I(SACprdX- SAC$y)^2))
SMCprdX <- sppred(SMC, condset= "X")
sqrt(mean(I(SMCprdX- SMC$y)^2))
SMCprdX <- sppred(SMC, newdata= exsmp$Dat.cal, condset= "X", power= T)
sqrt(mean(I(SMCprdX- SMC$y)^2))

## A ESSAYER AVEC L'AUTRE BASE SPATIALE
#+end_src

*** Conditioned on X, W

#+begin_src R :results output exemple
source("sppred.R")
SEMprdWX <- sppred(SEM, condset= "XW")
sqrt(mean(I(SEMprdWX- SEM$y)^2))
SXMprdWX <- sppred(SXM, condset= "XW")
sqrt(mean(I(SXMprdWX- SXM$y)^2))
SARprdWX <- sppred(SAR, condset= "XW")
sqrt(mean(I(SARprdWX- SAR$y)^2))
SDMprdWX <- sppred(SDM, condset= "XW")
sqrt(mean(I(SDMprdWX- SAR$y)^2))
SACprdWX <- sppred(SAC, condset= "XW")
sqrt(mean(I(SACprdWX- SAR$y)^2))
SMCprdWX <- sppred(SMC, condset= "XW")
sqrt(mean(I(SMCprdWX- SAR$y)^2))
SXMprdXW <- sppred(SXM, newdata= exsmp$Dat.cal,
                   condset= "XW", listw= exsmp$Wgt.cal)
sqrt(mean(I(SXMprdWX- SXM$y)^2))
#+end_src

*** KP1 predictors

#+begin_src R :results output exemple
source("sppred.R")
SEMprdKP1 <- sppred(SEM)
sqrt(mean(I(SEMprdKP1- SEM$y)^2))
SXMprdKP1 <- sppred(SXM)
sqrt(mean(I(SXMprdKP1- SXM$y)^2))
SARprdKP1 <- sppred(SAR)
sqrt(mean(I(SARprdKP1- SAR$y)^2))
SDMprdKP1 <- sppred(SDM)
sqrt(mean(I(SDMprdKP1- SAR$y)^2))
SACprdKP1 <- sppred(SAC)
sqrt(mean(I(SACprdKP1- SAR$y)^2))
SMCprdKP1 <- sppred(SMC)
sqrt(mean(I(SMCprdKP1- SAR$y)^2))

SXMprdKP1 <- sppred(SXM, newdata= exsmp$Dat.cal)
sqrt(mean(I(SXMprdKP1- SXM$y)^2))
SXMprdKP1 <- sppred(SXM, power= TRUE)
SXMprdKP1 <- sppred(SXM, newdata= exsmp$Dat.cal, listw= exsmp$Wgt.cal)
SXMprdKP1 <- sppred(SXM, newdata= exsmp$Dat.cal, listw= exsmp$Wgt.cal,
                    power= TRUE)
sqrt(mean(I(SXMprdKP1- SXM$y)^2))

#+end_src

*** Biased predictor

#+begin_src R :results output exemple
source("sppred.R")
SEMprdXWy <- sppred(SEM, condset= "XWy")
sqrt(mean(I(SEMprdXWy- SEM$y)^2))
SXMprdXWy <- sppred(SXM, condset= "XWy")
sqrt(mean(I(SXMprdXWy- SXM$y)^2))
SARprdXWy <- sppred(SAR, condset= "XWy")
sqrt(mean(I(SARprdXWy- SAR$y)^2))
SDMprdXWy <- sppred(SDM, condset= "XWy")
sqrt(mean(I(SDMprdXWy- SDM$y)^2))
SACprdXWy <- sppred(SAC, condset= "XWy")
sqrt(mean(I(SACprdXWy- SAC$y)^2))
SMCprdXWy <- sppred(SMC, condset= "XWy")
sqrt(mean(I(SMCprdXWy- SMC$y)^2))

SDMprdX <- sppred(SDM, condset= "XWy", power= TRUE)
SDMprdX <- sppred(SDM, condset= "XWy", newdata= exsmp$Dat.cal)
sqrt(mean(I(SDMprdXWy- SDM$y)^2))

## ON PEUT COMMENCER A METTRE DU yobs choisit
#+end_src

*** LSP predictor

#+begin_src R :results output exemple
source("sppred.R")
SEMprdKP1 <- sppred(SEM, blup= "LSP")
sqrt(mean(I(SEMprdKP1- SEM$y)^2))

SEMprdKP1 <- sppred(SEM, blup= "LSP", loo= TRUE)
sqrt(mean(I(SEMprdKP1- SEM$y)^2))


SXMprdXWy <- sppred(SXM, blup= "LSP", loo= TRUE)
sqrt(mean(I(SXMprdXWy- SXM$y)^2))

SARprdXWy <- sppred(SAR, condset= "XWy")
sqrt(mean(I(SARprdXWy- SAR$y)^2))
SDMprdXWy <- sppred(SDM, condset= "XWy")
sqrt(mean(I(SDMprdXWy- SDM$y)^2))
SACprdXWy <- sppred(SAC, condset= "XWy")
sqrt(mean(I(SACprdXWy- SAC$y)^2))
SMCprdXWy <- sppred(SMC, condset= "XWy")
sqrt(mean(I(SMCprdXWy- SMC$y)^2))

SDMprdX <- sppred(SDM, condset= "XWy", power= TRUE)
SDMprdX <- sppred(SDM, condset= "XWy", newdata= exsmp$Dat.cal)
sqrt(mean(I(SDMprdXWy- SDM$y)^2))

## ON PEUT COMMENCER A METTRE DU yobs choisit
#+end_src

*** KP2 predictor

#+begin_src R :results output exemple
source("sppred.R")
SEMprdKP2 <- sppred(SEM, blup= "KP2", loo= TRUE)
sqrt(mean(I(SEMprdKP2- SEM$y)^2))
SXMprdKP2 <- sppred(SXM, blup= "KP2", loo= TRUE)
sqrt(mean(I(SXMprdKP2- SXM$y)^2))
SARprdKP2 <- sppred(SAR, blup= "KP2", loo= TRUE)
sqrt(mean(I(SARprdKP2- SAR$y)^2))
SDMprdKP2 <- sppred(SDM, blup= "KP2", loo= TRUE)
sqrt(mean(I(SDMprdKP2- SDM$y)^2))
SACprdKP2 <- sppred(SAC, blup= "KP2", loo= TRUE)
sqrt(mean(I(SACprdKP2- SAC$y)^2))
SMCprdKP2 <- sppred(SMC, blup= "KP2", loo= TRUE)
sqrt(mean(I(SMCprdKP2- SMC$y)^2))

SMCprdKP2 <- sppred(SMC, condset= "X", blup= "KP2", loo= TRUE)
sqrt(mean(I(SMCprdKP2- SMC$y)^2))

## ON PEUT COMMENCER A METTRE DU yobs choisit
#+end_src


* Footnotes

[fn:1] This model has different names in the literature: spatial
autoregressive model with autoregressive disturbances (SARAR(1,1),
Kelejian and Prucha, 1998) or Spatial Autoregressive Conditional (SAC,
XX). We retain XX here.

